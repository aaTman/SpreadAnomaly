#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Mon Oct 30 18:19:24 2017

@author: taylorm
"""
import numpy as np
import sys
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from siphon import catalog, ncss
import requests
from netCDF4 import Dataset
import pdb
import gc

# The crontab killer when the run has already completed - the code inserts the
# run into a log file and this checks if it already completed.
def GEFScheck(init=0):
    if init == 1:
        url1 = 'http://thredds.ucar.edu/thredds/catalog/grib/NCEP/GEFS/Global_1p0deg_Ensemble/members/latest.html'

        #Play games with the url to get the TDSCatalog for siphon
        response = requests.get(url1)
        page = str(BeautifulSoup(response.content,'lxml'))
        start_link = page.find("a href")
    
        start_quote = page.find('"', start_link)
        end_quote = page.find('"', start_quote + 1)
        url = page[start_quote + 1: end_quote]
    
        url = url.split('html')
        endurl = url[1]
    
        # load in year and month as string
        year = endurl[83:87]
        month = endurl[87:89]
    
        # Load in run and day as string
        run = endurl[92:94]
        day = endurl[89:91]
    
        # Path to log
        pathCheck = '/home/taylorm/mcli/logs/'
     
        logC = open(pathCheck+'log','a')
        logC.write(run+' '+day+' '+month+' '+year+'\n')
        logC.close()
    else:
        #load the thredds latest GEFS run
        url1 = 'http://thredds.ucar.edu/thredds/catalog/grib/NCEP/GEFS/Global_1p0deg_Ensemble/members/latest.html'
    
        #Play games with the url to get the TDSCatalog for siphon
        response = requests.get(url1)
        page = str(BeautifulSoup(response.content,'lxml'))
        start_link = page.find("a href")
    
        start_quote = page.find('"', start_link)
        end_quote = page.find('"', start_quote + 1)
        url = page[start_quote + 1: end_quote]
    
        url = url.split('html')
        endurl = url[1]
    
        # load in year and month as string
        year = endurl[83:87]
        month = endurl[87:89]
    
        # Load in run and day as string
        run = endurl[92:94]
        day = endurl[89:91]
    
        # Path to log
        pathCheck = '/home/taylorm/mcli/logs/'
    
        # Checks if the log file has been made. If not, code will run.
        try:
            logCheck=open(pathCheck+'log')
        except IOError:
            print 'log file not created yet'
            return
    
        #write the details into the log to have a record of what runs were ran through the code
    
        # lastCheck is the log file text, testStr is the run from THREDDS
        lastCheck = logCheck.readlines()
        testStr = run+' '+day+' '+month+' '+year+'\n'
    
        print lastCheck[-1]
        print testStr
        # Quits python/the code if it is the same
        if lastCheck[-1]==testStr: #and os.listdir('/home/taylorm/ssa/mslp/me/') != []:
            sys.exit()



# Load in the timeArray from the m-climate file to prepare to subset the 21-day
# index over the 30 years (erratic index values due to leap years and invalid
# data).
def mcliTimeArray(time=None,var=None):
    # time is used to backtest if needed, var must be declared or else code
    # will error out
    print 'loading time array'
    if time == None:

        # location of my m-climate files

        if datetime.now().month == 11:
            varFile = Dataset('/home/taylorm/mcli/mclidata/mslpNovS.nc')
        else:
            varFile = Dataset('/home/taylorm/mcli/mclidata/'+var+'NH.nc')
        # gets the time variable from the netcdf
        varFile = varFile['time'][:]

        # converts array from ordinal time to python datetime instances
        timedt=[datetime(1800,1,1,0,0)+timedelta(hours=n) for n in varFile]

        return timedt

def mcliLoad(var=None,time=None,ind=None):
    print 'loading mcli array'
    if ind is not None:
        if var == 'mslp':
            if datetime.now().month == 12:
                varFile = Dataset('/home/taylorm/mcli/mclidata/mslpNovM.nc')
                varFileS = Dataset('/home/taylorm/mcli/mclidata/mslpNovS.nc')
                dataArr = varFile['Pressure'][ind,1:]
                dataArr = dataArr[...,::-1,:]
                dataArrS = varFileS['Pressure'][ind,1:]
                dataArrS = dataArrS[...,::-1,:]
                dataArr = np.expand_dims(dataArr,axis=0)
                dataArrS = np.expand_dims(dataArrS,axis=0)
                dataArr = np.append(dataArr,dataArrS,axis=0)
            else:
                varFile = Dataset('/home/taylorm/mcli/mclidata/'+var+'NH.nc')

        else:
            try:
                varFileS = Dataset('/home/taylorm/mcli/mclidata/'+var+'NHs.nc')
                varFileM = Dataset('/home/taylorm/mcli/mclidata/'+var+'NHm.nc')
                varFileS = varFileS['Temperature'][ind]
                varFileM = varFileM['Temperature'][ind]
                dataArr = np.expand_dims(varFileM,axis=0)
                dataArr = np.append(dataArr,varFileS,axis=0)
            except IOError:
                return
        ##Reforecast and GEFS have flipped lat axes, so I'm flipping the Reforecast's

#        if var is not 'mslp':
#            dataArr = np.squeeze(varFile['Temperature_isobaric'][ind])
#            varFileS = varFileS[...,::-1,:]
#            varFileM = varFileM[...,::-1,:]
#            dataArr = np.append(varFileS,varFileM,axis=2)
#            pdb.set_trace()


            #Haven't downloaded reforecast yet for temp
            #dataArr = np.squeeze(varFile[''])
        print 'loaded mcli array'
        gc.collect()
        return dataArr
    else:
        dataArr=0
        return dataArr


def mcliSub(dateList,fDate):

    years = [n.year for n in dateList]
    years = np.unique(years)

    val = [q for q in dateList for year in years if q-timedelta(days=10)<=fDate.replace(year=year)<=q+timedelta(days=10)]

    valset=set(val)
    ind = [i for i, item in enumerate(dateList) if item in valset]
    if len(ind) < 630:
        ind = None
    return ind

def pmm(ens):

    ensMean = np.mean(ens,axis=1)
    ensMeanSort = np.array([np.argsort(ensMean[i].flatten()) for i in range(0,len(ensMean))])
    ensMeanSortSort = np.array([np.argsort(ensMeanSort[i]) for i in range(0,len(ensMeanSort))])


    ensFlat = [ens[i].flatten() for i in range(0,len(ens))]
    ensFlat = np.array([ensFlat[i][0::21]for i in range(0,len(ens))])
    ensSort = np.array([np.argsort(ensFlat[i]) for i in range(0,len(ens))])


    enspmm = np.array([ensFlat[i][ensSort[i]][ensMeanSortSort[i]] for i in range(0,len(ensSort))])
    try:
        enspmm = enspmm.reshape((len(ensSort),len(ensMean[0]),len(ensMean[0,0])))
    except ValueError:

        pdb.set_trace()
    return enspmm

def GEFSload(wlon=180,elon=310,slat=20,nlat=80):
    #Initialize current time variable
    now = datetime.now()

    #load the thredds latest GEFS run
    url1 = 'http://thredds.ucar.edu/thredds/catalog/grib/NCEP/GEFS/Global_1p0deg_Ensemble/members/latest.html'

    #Play games with the url to get the TDSCatalog for siphon
    response = requests.get(url1)
    page = str(BeautifulSoup(response.content,'lxml'))
    start_link = page.find("a href")

    start_quote = page.find('"', start_link)
    end_quote = page.find('"', start_quote + 1)
    url = page[start_quote + 1: end_quote]

    url = url.split('html')
    endurl = url[1]

    #get the TDSCatalog file
    gefs = catalog.TDSCatalog(url1+endurl)
    ds = list(gefs.datasets.values())[0]

    #Pull the netcdf subset
    ncssl = ncss.NCSS(ds.access_urls['NetcdfSubset'])

    query = ncssl.query()
    #print the run that was loaded
    print 'obtained ' + endurl[83:94] + 'z run'

    #load in year and month as string
    year = endurl[83:87]
    month = endurl[87:89]

    print 'taking time range from ' + str(now)+ ' to ' +str(now+timedelta(days=6.75))

    #Load in run and day as string
    run = endurl[92:94]
    day = endurl[89:91]

    #take only the latlon box, netcdf4 file, and the mentioned variables, then assign them to variables
    query.lonlat_box(wlon,elon,slat,nlat).time_range(now,now+timedelta(days=7))

    query.accept('netcdf4')
    query.variables('Pressure_reduced_to_MSL_msl_ens','Temperature_isobaric_ens','Geopotential_height_isobaric_ens')
    data = ncssl.get_data(query)

    mslp = data.variables['Pressure_reduced_to_MSL_msl_ens'][:]
    lats = data.variables['lat'][:]
    lons = data.variables['lon'][:]
    try:
        dateArr = data.variables['time2'][:]
    except KeyError:
        try:
            dateArr = data.variables['time'][:]
        except KeyError:
            try:
                dateArr = data.variables['time1'][:]
            except KeyError:
                try:
                    dateArr = data.variables['time3'][:]
                except KeyError:
                    pdb.set_trace()

    tmps = data.variables['Temperature_isobaric_ens'][:]
    tmps = np.squeeze(tmps[:,:,np.where(data.variables['isobaric2'][:] == 85000),...])
    hgt = data.variables['Geopotential_height_isobaric_ens'][:]
    hgt = np.squeeze(hgt[:,:,np.where(data.variables['isobaric2'][:] == 50000),...])

    #pwat=data.variables['Precipitable_water_entire_atmosphere_single_layer_ens']
    #rh700 = data.variables['Relative_humidity_isobaric_ens']

    #take means and stdevs
    mslpMean = np.mean(mslp,axis=1)
    mslpStd = np.std(mslp,axis=1)
    mslpPMM = pmm(mslp)

    tmps = tmps-273.15
    tmpMean = np.mean(tmps,axis=1)
    tmpStd = np.std(tmps,axis=1)
    tmpPMM = pmm(tmps)

    hgtMean = np.mean(hgt,axis=1)
    hgtStd = np.std(hgt,axis=1)
    hgtPMM = pmm(hgt)

    #string of date and run
    date1 = year+month+day+run

    return dateArr, date1, mslpMean, mslpStd, mslpPMM,lats,lons, tmpMean, tmpStd, hgtMean, hgtStd, tmpPMM,hgtPMM
